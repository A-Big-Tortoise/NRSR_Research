{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import kurtosis, skew, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000) (20000,) 83.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([83., 79., 73., ..., 88., 87., 69.]),\n",
       " array([172., 173., 146., ..., 155., 159., 164.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = 100\n",
    "sampling_rate = 100\n",
    "duration = 10\n",
    "\n",
    "data_train = np.load('../Data/simu_20000_0.1_90_140_train.npy')\n",
    "data_test = np.load('../Data/simu_10000_0.1_141_178_test.npy')\n",
    "signals_train, S_train, D_train = data_train[:, :1000], data_train[:, -2], data_train[:, -1]\n",
    "signals_test, S_test, D_test = data_test[:, :1000], data_test[:, -2], data_test[:, -1]\n",
    "\n",
    "\n",
    "print(signals_test.shape, S_train.shape, D_test[0])\n",
    "D_test, S_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standize(data):\n",
    "    return (data - np.mean(data)) / np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_array(a, data_tmp):\n",
    "    i = 0\n",
    "    while i < len(a) - 2:\n",
    "        if data_tmp[a[i]] < data_tmp[a[i + 1]] < data_tmp[a[i + 2]]:\n",
    "            a = np.delete(a, i)\n",
    "        elif data_tmp[a[i]] > data_tmp[a[i + 1]] > data_tmp[a[i + 2]]:\n",
    "            a = np.delete(a, i + 2)\n",
    "        else:\n",
    "            i += 1\n",
    "    return a\n",
    "\n",
    "def get_peaks(data_tmp, duration, sampling_rate):\n",
    "    max = np.max(data_tmp)\n",
    "    data_tmp = data_tmp / max\n",
    "\n",
    "    t = np.linspace(0, duration, duration * sampling_rate)\n",
    "    signal = data_tmp\n",
    "\n",
    "    peak_indices, _ = find_peaks(signal) \n",
    "\n",
    "    t_peaks = t[peak_indices] \n",
    "    peak_values = signal[peak_indices]  \n",
    "    interpolation_func = interp1d(t_peaks, peak_values, kind='linear', bounds_error=False, fill_value=0)\n",
    "    envelope = interpolation_func(t)\n",
    "\n",
    "    peaks2, _ = find_peaks(envelope, distance=10)\n",
    "\n",
    "    peaks2 = update_array(peaks2, data_tmp)\n",
    "    if len(peaks2) % 2 != 0:\n",
    "        peaks2 = np.delete(peaks2, len(peaks2) - 1)\n",
    "\n",
    "    return peaks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_entropy(signal):\n",
    "    prob_distribution = np.histogram(signal, bins=10, density=True)[0]\n",
    "    ent = entropy(prob_distribution)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def calculate_centroid(signal):\n",
    "    centroid = np.sum(np.arange(len(signal)) * np.abs(signal)) / np.sum(np.abs(signal))\n",
    "    return centroid\n",
    "\n",
    "def calculate_spectral_entropy(signal, fs):\n",
    "    f, Pxx = welch(signal, fs=fs)\n",
    "    prob_distribution = Pxx / np.sum(Pxx)\n",
    "    spec_entropy = entropy(prob_distribution)\n",
    "    return spec_entropy\n",
    "\n",
    "\n",
    "def calculate_spectral_centroid(signal, fs):\n",
    "    f, Pxx = welch(signal, fs=fs)\n",
    "    spectral_centroid = np.sum(f * Pxx) / np.sum(Pxx)\n",
    "    return spectral_centroid\n",
    "\n",
    "\n",
    "# # Calculate Spectral Maximum Peaks\n",
    "# def calculate_spectral_max_peaks(signal, fs):\n",
    "#     f, Pxx = welch(signal, fs=fs)\n",
    "#     peaks, _ = find_peaks(Pxx)\n",
    "#     max_peaks = f[peaks]\n",
    "#     return max_peaks\n",
    "\n",
    "\n",
    "# Calculate Power Bandwidth\n",
    "def calculate_power_bandwidth(signal, fs):\n",
    "    f, Pxx = welch(signal, fs=fs)\n",
    "    total_power = np.sum(Pxx)\n",
    "    power_bandwidth = np.sum(Pxx >= total_power / 2)\n",
    "    return power_bandwidth\n",
    "\n",
    "\n",
    "def get_features(signal, peaks, fs):\n",
    "    \"\"\"\n",
    "    1. standize:\n",
    "    2. features:\n",
    "    \"\"\"\n",
    "    signal = standize(signal)\n",
    "    large_peaks_arr, small_peaks_arr = peaks[0::2], peaks[1::2]\n",
    "    large_amp_arr, small_amp_arr = signal[large_peaks_arr], signal[small_peaks_arr]\n",
    "    \n",
    "    large_amp, small_amp = np.mean(large_amp_arr), np.mean(small_amp_arr)\n",
    "    large_minus_small_amp = large_amp - small_amp\n",
    "    large_divide_by_small_amp = large_amp / small_amp\n",
    "\n",
    "    features_amp_list = [large_amp, small_amp, large_minus_small_amp, large_divide_by_small_amp]\n",
    "    \n",
    "    large2small_dis_arr = small_peaks_arr - large_peaks_arr\n",
    "    small2large_dis_arr = large_peaks_arr[1:] - small_peaks_arr[:-1]\n",
    "    \n",
    "    # print(large_peaks_arr)\n",
    "    # print(small_peaks_arr)\n",
    "    # print(large_peaks_arr[1:])\n",
    "    # print(small_peaks_arr[-1:])\n",
    "\n",
    "    large2large_dis_arr = np.diff(large_peaks_arr)\n",
    "\n",
    "    large2small_dis = np.mean(large2small_dis_arr)\n",
    "    small2large_dis = np.mean(small2large_dis_arr)\n",
    "    large2large_dis = np.mean(large2large_dis_arr)\n",
    "    large2small_divide_by_small2large_dis = large2small_dis / small2large_dis\n",
    "\n",
    "    features_dis_list = [large2small_dis, small2large_dis, large2large_dis,  large2small_divide_by_small2large_dis]\n",
    "\n",
    "    # # spectrum\n",
    "    # spec_entropy = calculate_spectral_entropy(signal, fs)\n",
    "    # spec_centroid = calculate_spectral_centroid(signal, fs)\n",
    "    # power_bandwidth = calculate_power_bandwidth(signal, fs)\n",
    "\n",
    "    # features_spec_list = [spec_entropy, spec_centroid, power_bandwidth]\n",
    "\n",
    "    # # others\n",
    "    # maximum = np.max(signal)\n",
    "    # minimum = np.min(signal)\n",
    "\n",
    "    # mean_abs_difference = np.mean(np.abs(np.diff(signal)))\n",
    "    # mean_difference = np.mean(np.diff(signal))\n",
    "    # sum_abs_difference = np.sum(np.abs(np.diff(signal)))\n",
    "    \n",
    "    # rms = np.sqrt(np.mean(np.square(signal)))\n",
    "    # entropy = calculate_entropy(signal)\n",
    "    # centroid = calculate_centroid(signal)\n",
    "    # kurt = kurtosis(signal)\n",
    "    # skeww = skew(signal)\n",
    "\n",
    "    # zero_crossing_rate = len(signal[signal > 0]) / len(signal)\n",
    "    # # energy = np.sum(signal ** 2)\n",
    "\n",
    "    # features_other_list = [maximum, minimum, mean_abs_difference, mean_difference, sum_abs_difference,\n",
    "    #                     rms, entropy, centroid, kurt, skeww,zero_crossing_rate]\n",
    "\n",
    "    # features_list = features_amp_list + features_dis_list + features_spec_list + features_other_list\n",
    "    features_list = features_amp_list + features_dis_list\n",
    "\n",
    "    return np.array(features_list) \n",
    "\n",
    "\n",
    "# signal = signals_train[0]\n",
    "# peaks = get_peaks(signal, duration, sampling_rate)\n",
    "# features = get_features(signal, peaks, fs)\n",
    "# features.shape\n",
    "\n",
    "\n",
    "# features_train, features_test = [], []\n",
    "\n",
    "# for signal in signals_train:\n",
    "#     peaks = get_peaks(signal, duration, sampling_rate)\n",
    "#     features = get_features(signal, peaks, fs)\n",
    "#     features_train.append(features)\n",
    "\n",
    "\n",
    "\n",
    "# for signal in signals_test:\n",
    "#     peaks = get_peaks(signal, duration, sampling_rate)\n",
    "#     features = get_features(signal, peaks, fs)\n",
    "#     features_test.append(features)\n",
    "\n",
    "# features_train = np.array(features_train)\n",
    "# features_test = np.array(features_test)\n",
    "\n",
    "# features_train.shape, features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../Data/features_train_23.npy', features_train)\n",
    "# np.save('../Data/features_test_23.npy', features_test)\n",
    "# features_train = np.load('../Data/features_train_23.npy')\n",
    "# features_test = np.load('../Data/features_test_23.npy')\n",
    "\n",
    "\n",
    "# np.save('../Data/features_train_8.npy', features_train)\n",
    "# np.save('../Data/features_test_8.npy', features_test)\n",
    "\n",
    "# features_train = np.load('../Data/features_train_8.npy')\n",
    "# features_test = np.load('../Data/features_test_8.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_unique(a):\n",
    "    mean = np.mean(a)\n",
    "    std = np.std(a)\n",
    "    # 设置阈值\n",
    "    threshold = 1\n",
    "    # 使用布尔索引删除特殊值\n",
    "    filtered_a = a[np.abs(a - mean) <= threshold * std]\n",
    "    return filtered_a\n",
    "\n",
    "def get_mag_0(peaks2, data_tmp):\n",
    "    return np.mean(data_tmp[peaks2[::2]])\n",
    "\n",
    "def get_mag_1(peaks2, data_tmp):\n",
    "    return np.mean(data_tmp[peaks2[1::2]])\n",
    "\n",
    "def get_features(data_tmp, peaks2):\n",
    "    max = np.max(data_tmp)\n",
    "    min = np.min(data_tmp)\n",
    "    data_tmp_raw = data_tmp\n",
    "    data_tmp = data_tmp / max\n",
    "\n",
    "    fs = 100\n",
    "\n",
    "    data_tmp = data_tmp_raw / max\n",
    "\n",
    "    diff_12 = peaks2[1::2] - peaks2[0::2]\n",
    "    diff_21 = peaks2[2::2] - peaks2[1:-1:2]\n",
    "    diff_22 = peaks2[2::2] - peaks2[:-2:2]\n",
    "\n",
    "    diff_12 = delete_unique(diff_12)\n",
    "    diff_21 = delete_unique(diff_21)\n",
    "    diff_22 = delete_unique(diff_22)\n",
    "\n",
    "    m_12 = np.mean(diff_12)\n",
    "    m_21 = np.mean(diff_21)\n",
    "    m_22 = np.mean(diff_22)\n",
    "\n",
    "    mag_0 = get_mag_0(peaks2, data_tmp_raw)\n",
    "    mag_1 = get_mag_1(peaks2, data_tmp_raw)\n",
    "\n",
    "    diff_12_mean = np.mean(diff_12)\n",
    "    diff_21_mean = np.mean(diff_21)\n",
    "\n",
    "    if diff_12_mean < diff_21_mean:\n",
    "        diff_min = diff_12_mean\n",
    "    else:\n",
    "        diff_min = diff_21_mean\n",
    "\n",
    "    diff_min = int(diff_min / 2)\n",
    "\n",
    "    kurt = []\n",
    "    for i in range(1, len(peaks2) - 1):\n",
    "        seg = data_tmp[peaks2[i] - diff_min:peaks2[i] + diff_min]\n",
    "        if len(seg) > 0:\n",
    "            kurt.append(kurtosis(seg))\n",
    "\n",
    "    kurt2 = kurt[::2]\n",
    "    kurt1 = kurt[1::2]\n",
    "\n",
    "    kurt2_mean = np.mean(kurt2)\n",
    "    kurt1_mean = np.mean(kurt1)\n",
    "\n",
    "    sk = []\n",
    "    for i in range(1, len(peaks2) - 1):\n",
    "        seg = data_tmp[peaks2[i] - diff_min:peaks2[i] + diff_min]\n",
    "        if len(seg) > 0:\n",
    "            sk.append(skew(seg))\n",
    "\n",
    "    skew2 = sk[::2]\n",
    "    skew1 = sk[1::2]\n",
    "\n",
    "    skew2_mean = np.mean(skew2)\n",
    "    skew1_mean = np.mean(skew1)\n",
    "\n",
    "    features = np.array([m_12, m_21, m_22, mag_0, mag_1, mag_0 / mag_1, kurt2_mean, kurt1_mean, skew1_mean, skew2_mean])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# features_train, features_test = [], []\n",
    "\n",
    "# for signal in signals_train:\n",
    "#     peaks = get_peaks(signal, duration, sampling_rate)\n",
    "#     features = get_features(signal, peaks)\n",
    "#     features_train.append(features)\n",
    "\n",
    "\n",
    "\n",
    "# for signal in signals_test:\n",
    "#     peaks = get_peaks(signal, duration, sampling_rate)\n",
    "#     features = get_features(signal, peaks)\n",
    "#     features_test.append(features)\n",
    "\n",
    "# features_train = np.array(features_train)\n",
    "# features_test = np.array(features_test)\n",
    "\n",
    "# np.save('../Data/features_train_10.npy', features_train)\n",
    "# np.save('../Data/features_test_10.npy', features_test)\n",
    "\n",
    "features_train = np.load('../Data/features_train_10.npy')\n",
    "features_test = np.load('../Data/features_test_10.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86189\\AppData\\Local\\Temp\\ipykernel_8512\\1710588305.py:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  features_train[:, 0] / features_train[:, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.69077397, 1.52926183, 0.45823817, ..., 1.88976928, 1.12906631,\n",
       "       1.11317292])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D12D21 = features_train[:, 0] / features_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86189\\AppData\\Local\\Temp\\ipykernel_8512\\3917738460.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  p1 = np.corrcoef(features_train[:, i], features_train[:, 0] / features_train[:, 1])[0, 1]\n",
      "d:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\numpy\\lib\\function_base.py:2674: RuntimeWarning: invalid value encountered in subtract\n",
      "  X -= avg[:, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b9046fac0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_train[0], S_train[0]\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "p1s = []\n",
    "p2s = []\n",
    "for i in range(len(features_train[0])):\n",
    "    # print(i)\n",
    "    p1 = np.corrcoef(features_train[:, i], features_train[:, 0] / features_train[:, 1])[0, 1]\n",
    "    print(p1)\n",
    "    p1s.append(p1)\n",
    "    # p2 = np.corrcoef(features_train[:, i], D_train)[0, 1]\n",
    "    # p2s.append(p2)\n",
    "    # print(p1, p2)\n",
    "plt.plot(p1s)\n",
    "# 计算皮尔逊相关系数\n",
    "# correlation_coefficient = np.corrcoef(features_train[:, 1], S_train)[0, 1]\n",
    "\n",
    "# 绘制散点图\n",
    "# plt.scatter(, y, label=f\"Pearson's r = {correlation_coefficient:.2f}\")\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.title('Scatter Plot with Pearson Correlation')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], Loss: 12.7666\n",
      "Epoch [10/1000], Loss: 12.7700\n",
      "Epoch [15/1000], Loss: 12.7644\n",
      "Epoch [20/1000], Loss: 12.7722\n",
      "Epoch [25/1000], Loss: 12.7744\n",
      "Epoch [30/1000], Loss: 12.7747\n",
      "Epoch [35/1000], Loss: 12.7638\n",
      "Epoch [40/1000], Loss: 12.7624\n",
      "Epoch [45/1000], Loss: 12.7723\n",
      "Epoch [50/1000], Loss: 12.7763\n",
      "Epoch [55/1000], Loss: 12.7797\n",
      "Epoch [60/1000], Loss: 12.7660\n",
      "Epoch [65/1000], Loss: 12.7676\n",
      "Epoch [70/1000], Loss: 12.7772\n",
      "Epoch [75/1000], Loss: 12.7706\n",
      "Epoch [80/1000], Loss: 12.7708\n",
      "Epoch [85/1000], Loss: 12.7705\n",
      "Epoch [90/1000], Loss: 12.7720\n",
      "Epoch [95/1000], Loss: 12.7782\n",
      "Epoch [100/1000], Loss: 12.7672\n",
      "Epoch [105/1000], Loss: 12.7756\n",
      "Epoch [110/1000], Loss: 12.7593\n",
      "Epoch [115/1000], Loss: 12.7615\n",
      "Epoch [120/1000], Loss: 12.7649\n",
      "Epoch [125/1000], Loss: 12.7657\n",
      "Epoch [130/1000], Loss: 12.7673\n",
      "Epoch [135/1000], Loss: 12.7706\n",
      "Epoch [140/1000], Loss: 12.7670\n",
      "Epoch [145/1000], Loss: 12.7704\n",
      "Epoch [150/1000], Loss: 12.7624\n",
      "Epoch [155/1000], Loss: 12.7710\n",
      "Epoch [160/1000], Loss: 12.7667\n",
      "Epoch [165/1000], Loss: 12.7528\n",
      "Epoch [170/1000], Loss: 12.7617\n",
      "Epoch [175/1000], Loss: 12.7757\n",
      "Epoch [180/1000], Loss: 12.7676\n",
      "Epoch [185/1000], Loss: 12.7686\n",
      "Epoch [190/1000], Loss: 12.7746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PHD\\Research\\Code\\features.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PHD/Research/Code/features.ipynb#X10sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PHD/Research/Code/features.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PHD/Research/Code/features.ipynb#X10sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()    \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PHD/Research/Code/features.ipynb#X10sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PHD/Research/Code/features.ipynb#X10sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(losses)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    305\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m    309\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "input_size = 10\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features_train)\n",
    "features_train = scaler.transform(features_train)\n",
    "features_test = scaler.transform(features_test)\n",
    "\n",
    "features_train_ft = torch.FloatTensor(features_train[:, :input_size])\n",
    "features_test_ft = torch.FloatTensor(features_test[:, :input_size])\n",
    "\n",
    "S_train_ft = torch.FloatTensor(S_train)\n",
    "S_test_ft = torch.FloatTensor(S_test)\n",
    "D_train_ft = torch.FloatTensor(D_train)\n",
    "D_test_ft = torch.FloatTensor(D_test)\n",
    "\n",
    "train_dataset_S = TensorDataset(features_train_ft, S_train_ft)\n",
    "train_dataset_D = TensorDataset(features_train_ft, D_train_ft)\n",
    "test_dataset_S = TensorDataset(features_test_ft, S_test_ft)\n",
    "test_dataset_D = TensorDataset(features_test_ft, D_test_ft)\n",
    "\n",
    "train_dataset = train_dataset_S\n",
    "test_dataset = test_dataset_S\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden2 = nn.Linear(hidden_size1, hidden_size1)\n",
    "        self.hidden3 = nn.Linear(hidden_size1, hidden_size1)\n",
    "        self.hidden4 = nn.Linear(hidden_size1, hidden_size1)\n",
    "        self.hidden5 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.hidden1(x))\n",
    "        x = self.sigmoid(self.hidden2(x))\n",
    "        x = self.sigmoid(self.hidden3(x))\n",
    "        x = self.hidden4(x)\n",
    "        x = self.hidden5(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# input_size = 23\n",
    "hidden_size1 = input_size \n",
    "hidden_size2 = input_size // 2\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "model = SimpleMLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for data, label in train_loader:\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, label)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(losses):.4f}')\n",
    "\n",
    "# with torch.no_grad():\n",
    "    # predictions = model(x_test)\n",
    "\n",
    "# print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
